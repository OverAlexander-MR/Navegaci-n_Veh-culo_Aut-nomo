\documentclass[lettersize, journal]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
%Necesario para el parametro H en las figuras 
\usepackage{float}
% Numeros periodicos
\usepackage{yhmath}
\usepackage{fontawesome5}
\usepackage{csquotes} %Relación con el idimoa
\usepackage[backend=biber, style=ieee]{biblatex}
\renewcommand{\bibfont}{\small}
\usepackage{xr}
\usepackage{cuted}
\usepackage{caption}
\usepackage{enumitem}
\externaldocument{suplemento}
\usepackage{xcolor}
\usepackage{subcaption}  % Para subfiguras
\usepackage{booktabs}
\usepackage{hhline}



\usepackage{multirow}
\usepackage{colortbl}
\usepackage{array}


%\renewcommand{\tablename}{Tabla}
\captionsetup[table]{name=Tabla} 
\newcommand{\jph}[1]{{\color{blue}{#1}}}

\bibliography{Referencias}

\begin{document}

%\title{Sistema de navegación adaptativa para un vehículo autónomo en un entorno con establecimiento objetivo}
\title{Adaptive Navigation System for an Autonomous Vehicle in a Goal-Oriented Environment}
\definecolor{orcidcolor}{HTML}{A6CE39}

\author{Over Alexander Mejia-Rosado \textsuperscript{\href{https://orcid.org/0009-0008-8152-2754}{\textcolor{orcidcolor}{\faOrcid}}}, 
Ronald Mateo Ceballos Lozano \textsuperscript{\href{https://orcid.org/0009-0003-6478-3286}{\textcolor{orcidcolor}{\faOrcid}}}, 
Rhonald José Torres Diaz \textsuperscript{\href{mailto:rhtorresd@unal.edu.co}{\textcolor{orcidcolor}{\faIcon{envelope}}}}, y 
Juan Pablo Hoyos Sanchez \textsuperscript{\href{mailto:jhoyoss@unal.edu.co}{\textcolor{orcidcolor}{\faIcon{envelope}}}}

\thanks{Over Alexander Mejia-Rosado (omejiar@unal.edu.co), Ronald Mateo Ceballos Lozano (rceballosl@unal.edu.co), Rhonald José Torres Diaz (rhtorresd@unal.edu.co) and Juan P. Hoyos (e-mail: jhoyoss@unal.edu.co) are with Universidad Nacional de Colombia, sede De La Paz, Colombia.}%
}

\maketitle

\begin{abstract}
Este proyecto se centra en el desarrollo de un sistema de navegación para vehículos autónomos utilizando NEAT (NeuroEvolution of Augmenting Topologies). El objetivo principal es diseñar e implementar un vehículo capaz de desplazarse en un mapa en 2D, el cual consta de un punto inical y un objetivo para el vehiculo autonomo. A través de sensores virtuales, el vehículo es capaz de identificar la vía por la cual puede avanzar y cuales los los limites de esta. Se utilizan métricas de distancia como la Euclidiana, Manhattan y Chebyshev cómo sistema de recompensas, que constantemente calculan las posiciones de los agentes, entre más cercano esté el vehiculo del objetivo, su fitness incrementa, de esa manera se esttablece la función aptitud. Se crea y se implementa un sistema de refuerzo forzado, que permite al vehículo avanzar en caso de que su velocidad sea menor a 0.1, evitando así que el vehículo se estanque. Los resultados obtenidos muestran que el vehículo autónomo es capaz de desplazarse de manera autónoma en el mapa, aumentando su aptitud en cada generación dependiendo de la metrica de distancia utilizada.
\end{abstract}

\begin{IEEEkeywords}
NEAT, autonomous vehicle, fitness function, distance metrics, reinforcement learning.
\end{IEEEkeywords}

\section{Introducción}
\IEEEPARstart{U}{no} de los mayores problemas del transporte moderno es el alto indice de accidentes en el tránsito terrestre, la gran cantidad de trafico atribuida a factores como errores humanos, falla mecánicas y fenómenos naturales. Según la Organización Mundial de la Salud (OMS) en 2023, entre 2010 y 2021 el número de vehículos a nivel mundial se duplicó, superando los mil millones \cite{Accidentes}, con 1,19 millones de muertes en todo el mundo, y una tasa de mortalidad de 15 personas por cada 100,000 habitantes. Además, cerca del 80\% de las vías de tránsito a nivel mundial no cumplen con los estándares básicos de seguridad para peatones y ciclistas, lo que incrementa el riesgo para ciclistas y peatones, especialmente en entornos urbanos con alta densidad de tráfico, generando congestión y emisiones contaminantes. \cite{montezuma2008derecho, diaz2015infraestructuras}.

%Estos factores resaltan la necesidad de intervenciones más efectivas, como la automatización y los sistemas inteligentes de tráfico, que permitan reducir las cifras de mortalidad y mejorar la eficiencia del tránsito mediante sistemas coordinados.

%Sistemas autonomonos
%\subsection{Sistemas Autónomos}
La automatización y los sistemas inteligentes de tráfico surgen como alternativas efectivas para reducir accidentes y optimizar el tránsito. Estos sistemas, mejoran la seguridad y eficiencia tanto del vehículo como del transporte en general, enfocándose no solo en el control preciso del automóvil, sino también en la capacidad del sistema para adaptarse dinámicamente a condiciones variables y mantener una trayectoria segura para los pasajeros y otros agentes viales \cite{arshad2020clothoid}. Los vehículos autónomos, apoyados en tecnologías avanzadas como la Programación Dinámica Aproximada (ADP), mejoran la seguridad y eficiencia al adaptarse a condiciones variables, optimizando trayectorias y minimizando errores de seguimiento. Este enfoque integra técnicas avanzadas de inteligencia artificial para garantizar un control preciso y decisiones en tiempo real  \cite{ADP_A}.
%El control de movimiento y la precisión en el seguimiento de trayectoria son aspectos críticos para la evaluación del agente inteligente a bordo del vehículo, siempre con el objetivo de preservar la integridad de las personas dentro del automóvil. El control de movimiento en vehículos autónomos, en especial el control óptimo de seguimiento de trayectoria busca mantener al vehículo dentro de los limites de seguridad establecidos, optimizando los movimientos del automóvil y ajustando el comportamiento del vehículo a través de inteligencia artificial avanzada, como la ADP (Adaptive Dynamic Programming), o Approximate Dynamic Programming (Programación dinámica aproximada) \cite{ADP_A}. La ADP es una técnica de control avanzada utilizada en vehículos autónomos para la toma de decisiones en tiempo real bajo condiciones de incertidumbre, y con tecnología de control basada en el aprendizaje por refuerzo, el uso de programación dinámica adaptativa critica, aumenta el rendimiento del sistema \cite{Smart_Car_ADP, ADP_Inf}. La ADP utiliza una función de costo que evalúa el rendimiento del control del vehículo, minimizandose esta función mediante procesos iterativos, lo que le permite ajustar continuamente las acciones para optimizar el rendimiento y reducir errores de seguimiento en la trayectoria. En \cite{Smart_Car_ADP} se propone un método de control óptimo adaptativo con rendimiento prescrito para resolver el problema del control de seguimiento de trayectorias para vehículos autónomos con dinámica inercial, y al introducir una función de rendimiento prescrito (PPF) en la programación dinámica adaptativa (ADP), el controlador puede restringir el error de seguimiento del sistema dentro de un límite de rendimiento especificado al tiempo que optimiza el costo de control.
%El aprendizaje por refuerzo que ha demostrado ser eficaz en la toma de decisiones en vehículos autónomos es el Deep Q-Network (DQN). En este enfoque, el agente aprende a tomar decisiones a través de la interacción con su entorno, recibiendo recompensas o penalizaciones en función de sus acciones, lo que mejora su aprendizaje \cite{DQN_Inf}. La técnica DQN utiliza funciones que estiman el valor de las acciones en los diferentes estados en los que se encuentra el agente, representando así la expectativa de recompensa acumulada al realizar una acción específica desde un estado particular.
%Además, DQN implementa un balance entre exploración y explotación. La exploración implica probar nuevas acciones, mientras que la explotación se refiere a elegir acciones que el agente considera "buenas". Este balance se evalúa mediante el mecanismo epsilon-greedy, donde el agente elige acciones aleatorias con una probabilidad $\epsilon$ que varía entre 0 y 1 \cite{atari}. Un valor alto de $\epsilon$ permite una mayor exploración, mientras que un valor bajo indica una mayor explotación de las acciones ya aprendidas. Sin embargo, debido a que el operador máximo en DQN selecciona el valor máximo para evaluar una acción producida por el mismo valor Q, existe el riesgo de obtener valores estimados de manera demasiado optimista. Para mitigar este problema, se propone disociar la selección de la evaluación, lo que da lugar al enfoque Double DQN \cite{Double-DQN}. En este método, se utilizan dos funciones de valor (redes Q) que se aprenden actualizando alternativamente una de ellas, resultando en dos conjuntos de pesos de red: $w$ y \( w^{\prime} \). La función de valor con peso $w$ se utiliza para determinar la acción a tomar, de acuerdo con una política voraz, mientras que la función con peso \( w^{\prime} \) se utiliza para evaluar el valor Q.
%DQN puede ser implementado en los casos donde se requiera alta precisión,  cómo por ejemplo la construcción mapas. También aplicar el DQN en la conducción autónoma y el transporte inteligente, mejorando así la trazabilidad de las trayectorias en vehículos autónomos \cite{Smart_Car_DQN}.
%El Deep Deterministic Policy Gradient (DDPG), es otro algoritmo de aprendizaje por refuerzo diseñado para resolver problemas de control continuo, este dependiendo del entorno, puede aumentar la optimización del sistema, agregando un mayor indice de credibilidad al algoritmo \cite{movile}.
%Por otro lado se encuentra la PPO (Proximal Policy Optimization) una técnica de aprendizaje por refuerzo avanzada y popular utilizada en entornos complejos de sistemas de vehículos autónomos. La PPO pertenece a los métodos de optimización de políticas, lo que significa que se centra en las estrategias a seguir con respecto al entorno dinámico donde se encuentra el agente. Esta técnica introduce una optimización proximal para limitar cuanto se puede actualizar la política en cada paso de entrenamiento, limitando así los grandes cambios en las políticas y estabilizando el aprendizaje para que el agente sea consistente \cite{PPO_Inf}. A diferencia de los métodos basados en funciones Q, PPO aprende directamente una política estocástica, lo que significa que genera probabilidades para cada acción en lugar de seleccionar siempre la misma acción en un estado determinado. Este implementa un algoritmo $actor-critic$ donde el actor aprende la política (qué acción tomar en cada estado) y  el critic estima el valor de cada estado (cuál es la recompensa esperada en un estado dado). Esta técnica se ha utilizado en el siguiente articulo \cite{Smart_Car_PPO} donde se presento un enfoque basado en MAPPO (Multi-Agent Proximal Policy Optimization) para garantizar la maniobra segura y eficiente de vehículos autónomos en presencia de vehículos de emergencias. El método propuesto genera políticas cooperativas que permiten al vehículo de emergencia circular a una velocidad media un 15\% superior manteniendo unas distancias de seguridad elevadas.
%Tambien se tiene el enfoque de Soft Actor-Critic (SAC) que combina dos características avanzadas en aprendizaje por refuerzo profundo: aprendizaje fuera de línea y ajuste automático de la función de valor.  El SAC se aplica para maximizar tanto la recompensa acumulada como la entropía de las acciones, lo que ayuda a gestionar entornos complejos y requisitos de control variables. En concreto, el algoritmo incluye un componente de política estocástica, que beneficia la planificación de la trayectoria del agente autónomo al mejorar la adaptabilidad de las acciones de control en entornos inciertos y dinámicos. El enfoque basado en SAC también utiliza una estructura de recompensa compuesta para equilibrar múltiples objetivos de optimización, como la eficiencia del controlador, la solidez y el uso de energía, lo que mejora el rendimiento general y la precisión de la navegación \cite{SAC}.  Este cuenta con distintas variantes, cómo lo es el LGE-SAC, este introduce una variante en el manejo de experiencias, enfocándose en aprender de las experiencias positivas en la memoria de repetición de experiencias. También existe el  RSAC, este incrementa la optimización a través de su adaptabilidad debido al manejo complejo recompensas, permite a los agentes aprender de experiencias de mayor calidad de manera más rápida, optimizando el proceso de aprendizaje en entornos complejos \cite{RAS}.
%La adaptabilidad del sistema es esencial para la precisión y la velocidad en la toma de deciciones, esto es aplicado en la búsqueda de rutas que se han convertido en factores clave para medir el rendimiento de un AGV (Vehículo de Guiado Automático) en un RMFS (Sistema de Manejo de Materiales en Tiempo Real). El algoritmo de Dijkstra es un método clásico para la búsqueda de rutas, ya que determina la trayectoria disponible desde el nodo de inicio hasta el nodo de destino explorando todos los nodos posibles. Por su parte, el algoritmo A* es una extensión del algoritmo de Dijkstra que mejora el rendimiento al utilizar heurísticas para guiar su búsqueda. Sin embargo, la exploración y el registro de todos los nodos posibles pueden requerir mucho tiempo, especialmente en sistemas de gran tamaño. En este contexto, el aprendizaje por refuerzo profundo se presenta como una solución prometedora para resolver problemas de búsqueda de rutas. A través de una red neuronal entrenada, el AGV puede tomar decisiones informadas basadas en la situación del sistema y determinar la ruta más adecuada. Para la planificación de rutas de un solo AGV, se puede emplear un algoritmo de aprendizaje Q, que es un enfoque basado en valores. Para escenarios que involucran múltiples AGV, se puede utilizar un algoritmo de aprendizaje por refuerzo conocido como Actor-Critic, que ayuda a encontrar rutas libres de conflictos entre varios vehículos \cite{A_dqn}.
%Sin duda, la evolución de los sistemas de transporte (TS) y la aparición de vehículos autónomos (VA) marcan avances fundamentales que configuran las ciudades modernas. Sin embargo, los VA enfrentan importantes desafíos en entornos complejos e impredecibles. 
%El transporte urbano ha evolucionado significativamente, enfatizando la eficiencia, la seguridad y los avances en TS para abordar la rápida expansión urbana. A pesar de estos progresos, la complejidad de lograr sistemas completamente autónomos sigue siendo un reto.
%Esta complejidad se origina en escenarios de tráfico intrincados, la necesidad de sistemas robustos de prevención de colisiones y la integración de tecnologías impulsadas por inteligencia artificial (IA) para una toma de decisiones óptima \cite{Smart_Agent,Reinfor_Control,traffic_lights}. %En este contexto, la investigación destaca el papel del aprendizaje por refuerzo (RL) como una herramienta fundamental para permitir que los VA naveguen en escenarios de tráfico inciertos y complicados. El RL capacita a los VA para adaptarse y tomar decisiones óptimas a través de un proceso de aprendizaje basado en prueba y error, mejorando significativamente su resiliencia y confiabilidad en situaciones impredecibles.
%Por ejemplo, el aprendizaje automático, ha demostrado capacidades para gestionar la detección de objetos, rastrea múltiples entidades y pronostica posibles escenarios en el entorno del vehículo. La integración de algoritmos de aprendizaje por refuerzo profundo (DRL) ha sido útil para optimizar el uso de los sensores de los vehículos autónomos, mejorando sus capacidades de conducción \cite{dqn_va}. La combinación de todas estas técnicas, elaboradas dentro de una arquitectura clásica de control, podría facilitar una navegación autónoma eficiente que priorice el cumplimiento de la ruta, evite colisiones y se adapte en tiempo real a situaciones dinámicas.

Por otro lado, algoritmos clásicos como Dijkstra y A* se complementan con aprendizaje por refuerzo profundo para planificación de rutas más eficiente \cite{Smart_Agent,Reinfor_Control,traffic_lights,A_dqn}, mientras que la Neuro-Evolución de Topologías Aumentadas (NEAT) optimiza redes neuronales adaptativas, destacándose como una herramienta poderosa para abordar los retos asociados. NEAT ofrece una potente herramienta de aprendizaje evolutivo en la creación de redes neuronales artificiales (ANN), utilizando algoritmos genéticos (GA) para optimizar las redes neuronales, aumentando gradualmente su complejidad según la necesidad de la tarea. Por ejemplo, en el estudio presentado en \cite{NEAT}, se analiza un modelo de simulación de evacuación basado en agentes, en el cual se estudia la dinámica de diferentes vehículos y su proceso de aprendizaje mediante NEAT. Este enfoque refuerza la analogía entre los GA y la evolución biológica, permitiendo que las redes neuronales evolucionen de manera adaptativa para afrontar entornos dinámicos y optimizar soluciones de forma simultánea.

El rendimiento de los algoritmos de Neuro-Evolución se compara favorablemente con el de los algoritmos de retropropagación basados en gradientes. Una característica clave de NEAT es que el proceso de evolución comienza a partir de redes neuronales muy simples, cuya topología aumenta gradualmente en complejidad a lo largo de la evolución\cite{NEAT_2}. Esta característica reduce la complejidad innecesaria de la red neuronal final, algo que no es posible lograr utilizando algoritmos basados en gradientes.
Este trabajo examinó cómo el algoritmo NEAT (NeuroEvolution of Augmenting Topologies) permite la optimización evolutiva de redes neuronales mediante topologías dinámicas y mutaciones estructurales. La implementación de NEAT aborda problemas complejos como la clasificación y el control en tiempo real, empleando especiación y un registro histórico de innovaciones para mantener la diversidad y mejorar la precisión sin ajuste manual. también implementando  algoritmos evolutivos como Evolutionary Acquisition of Neural Topologies, (EANT) los cuales pueden superar NEAT en temas  como el rendimiento \cite{NAT_R}.

Este trabajo desarrolla un sistema de navegación para vehículos autónomos basado en NEAT (NeuroEvolution of Augmenting Topologies). El vehículo, equipado con sensores virtuales, navega en un mapa 2D desde un punto inicial hasta un objetivo, utilizando métricas de distancia (Euclidiana, Manhattan y Chebyshev) como función de aptitud para evaluar y mejorar su desempeño. Para evitar estancamientos, se implementa un refuerzo forzado que asegura movimiento constante. Los resultados muestran que el vehículo mejora su capacidad de navegación autónoma con cada generación.

%\subsection{Sistemas Multi-Agentes}
%Uno de los mayores desafíos para los vehículos autónomos es la capacidad de navegar de forma segura en entornos con agentes aleatorios, como intersecciones donde otros vehículos y peatones pueden cambiar de dirección de forma impredecible. El manejo seguro de intersecciones es esencial, ya que estos puntos concentran altos índices de accidentes y congestión vehicular, incrementando el tiempo de espera y el consumo de combustible en áreas urbanas densas.
%\\
%Este articulo se inspira en soluciones basadas en sistemas multi-agente, que emplean aprendizaje por refuerzo y redes neuronales profundas para coordinar el tráfico en intersecciones, permitiendo a los vehículos autónomos pasar de forma segura y eficiente sin intervención humana \cite{Smart_Agent}. Esta técnica aporta mayor adaptabilidad y seguridad en comparación con sistemas estáticos, que requieren ajustes específicos y no consideran agentes móviles inesperados.
%\\
%Implementacion de mapas
 %\cite{Reinfor_Control}
 %\cite{traffic_lights}
%La implementación de estos sistemas autónomos incorpora herramientas de retroalimentación como el aprendizaje reforzado y redes neuronales, que permiten evaluar cada acción tomada por el agente inteligente y ajustar parámetros para mejorar las decisiones y alcanzar los objetivos propuestos \cite{Smart_Car}.

\section{Metodología}
Para la construcción del sistema de navegación adaptativa, se uso el entorno con la librería \href{https://www.pygame.org/wiki/GettingStarted}{Pygame} de python. Se asigna un mapa y un agente principal (Vehículo Autónomo). El entorno implementado es una fracción del mapa clásico del video juego \href{https://www.rockstargames.com/games/gta2}{GTAII}, además la imagen del agente, y parámetros iniciales de configuración es proporcionada por el repositorio \href{https://github.com/NeuralNine/ai-car-simulation}{NeuralNine}, también se hace uso de algunas imagenes que pertenecen al repositorio \href{https://github.com/mihir-m-gandhi/Traffic-Intersection-Simulation-with-Stats}{Mihir Gandhi}.
Se establecen los estados, un estado inicial donde se ubica el vehículo autónomo y un estado final. Se asignan pixeles verdes RBG (0, 0, 255) al mapa, que será el objetivo.
% \begin{figure}[H]
%     \centering    \includegraphics[width=0.3\linewidth]{images/car.png}
%     \caption{Agente previo a la aplicación de la red neuronal}
%     \label{fig:agente}
% \end{figure}


\subsection{NEAT-Python}
El agente debe aprender a moverse en el entorno, NEAT permite que el agente recorra el espacio asignándoles valores de salida dependiendo de los valores de entrada. Los valores de entrada son valores que se toman del entorno, para este caso en particular, las entradas estarían dadas por el color de cada pixel, dependiendo del color del pixel, el agente cambia o no de dirección,  a esto se le conoce como acción o salida. NEAT le otorga al agente sensores que le permiten el reconocimiento del entrono, estos sensores son asignados para determinar cuando este sale de las vías, y la distancias entre puntos \cite{sensores}. En este articulo implementamos el uso de 5 sensores, asignados a una lista; -90°, -45,° 0°, 45° y 90°. Estas son las principales entradas para nuestro algoritmo NEAT aplicado al agente ver Figura \ref{fig:agente_sensores}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{images/sensores.png}
    \caption{Asignación de sensores al agente}
    \label{fig:agente_sensores}
\end{figure}

En la Figura \ref{fig:Red-neuronal},  Hidden o capas ocultas, son nodos que aplican un procesado a la entrada, con cada generación las capas aumentan dependiendo de la ganancia, esta ganancia está dada por las acciones previas del agente. Si el agente se desplaza en el mapa sin exceder las limitaciones, se mantiene en la generación. Las generaciones en NEAT es el umbral con el que se desea finalizar la simulación, si se establecen 50 generaciones cada valor del umbral se le asocian a los agentes creados por la red neural aplicando distintos valores de configuración, si en la primera generación todos los carros chocan, empeiza a la siguiente generación,así sucesivamente hasta llegar a la ultima que sería la generación 50. En el entrono se crean distintos agentes, cada agente recorre el mapa, si un agente choca o se sale de la vía, esta configuración tendrá menos probabilidad de replicarse.

En la configuración inicial, Hidden es igual a 0, puesto que se inicia con una configuración sencilla, con cada generación Hidden aumenta para que la salida dada la entrada sea la mas óptima.

\begin{figure}
    \centering    \includegraphics[width=0.4\linewidth]{images/neural_network.png}
    \caption{Estructura general de una red neuronal multicapa \cite{red_neuronal}}
    \label{fig:Red-neuronal}
\end{figure}

\subsection{Fitness function}
Para evaluar que tan bien son los genomas (próximas generaciones) NEAT implementa una función que evalúa que tan bien se resuelve el problema en cuestión, esta función es llamada \textit{Fitness function}. Si un genoma A resuelve el problema de manera más exitosa que un genoma B, entonces el valor de aptitud de A debe ser mayor que el de B. La magnitud absoluta y los signos de estas aptitudes no son importantes, solo sus valores relativos.
% , los pasos estarían dados por:
% \begin{itemize}
%     \item Crear una red neuronal: Basada en el genoma.
%     \item Proveer entradas y calcular salidas: Por ejemplo, para cada caso en la tabla de verdad del XOR, se proporcionan las entradas a la red y se calcula la salida.
%     \item Calcular el error: El error se calcula entre las salidas esperadas y las salidas reales de la red.
%     \item Asignar aptitud: Si la red produce exactamente la salida esperada, su aptitud es 1. De lo contrario, es un valor menor a 1, disminuyendo más cuanto más incorrectas sean las respuestas de la red \cite{neat_python}.
% \end{itemize}

La ecuación de actitud sería:
\begin{align*}
    Aptitud = 1 - \sum_{i} (e_i - a_i)^2,
\end{align*}
donde, $e_i$ (salidas esperadas): Son los valores que se esperan obtener de la red neuronal o agente autónomo. Las salidas esperadas son las posiciones ideales o los movimientos óptimos que el agente debería realizar para alcanzar el objetivo de la manera más eficiente posible. 

$a_i$ (salidas reales): Son los valores que realmente se obtienen de la red neuronal o agente autónomo. 
% Estas son las posiciones o movimientos que el agente realiza durante su desplazamiento en el mapa.

%%%
\textit{Función de Aptitud}. En el contexto de nuestro proyecto, la función de aptitud es fundamental para evaluar y guiar el proceso de evolución de los agentes autónomos controlados por redes neuronales generadas mediante el algoritmo NEAT. 

Los pasos para calcular la aptitud de cada genoma son los siguientes:

\begin{itemize} 
\item \textbf{Crear una red neuronal}: Se genera una red neuronal basada en el genoma actual. 
\item \textbf{Simular el agente}: La red neuronal controla al agente autónomo en el entorno simulado (ver Figura \ref{fig:Mapa}), recibiendo entradas del entorno (como las distancias detectadas por los sensores del agente) y produciendo acciones, cambios en la dirección y velocidad. 
\item \textbf{Recopilar métricas}: Durante la simulación, se registran métricas relevantes, como la distancia mínima al objetivo, el tiempo de supervivencia y si el agente ha colisionado. 
\item \textbf{Calcular la aptitud}: Se utiliza una función de aptitud definida para calcular el valor de aptitud del genoma, basándose en las métricas recopiladas. 
La función de aptitud implementada es:
\begin{align*} 
    \text{Aptitud} = R_d
\end{align*}
donde $R_d$ es la recompensa por distancia al objetivo. 

\item \textbf{Recompensa por distancia al objetivo ($R_d$)}:

La recompensa por distancia es inversamente proporcional a la distancia $d_{agent}$ entre el agente y el objetivo:

\begin{align*} 
R_d = \begin{cases} 
10000, \quad \text{si } \quad d_{agent} = 0 
\cr 10000-d_{agent}, \quad \text{si} \quad d_{agent} > 0 
\end{cases} 
\end{align*}

Donde para la distancia Euclidiana $d_{agent}$ se calcula como:
\begin{align*} 
    d_{agent} = {\sqrt{(x_{agent} - x_{goal})^2 + (y_{agent} - y_{goal})^2}} 
\end{align*}

Para la distancia de Manhattan $d_{agent}$ se calcula como:
\begin{align*} 
    d_{agent} = |x_{agent} - x_{goal}| + |y_{agent} - y_{goal}|
\end{align*}

El tercer y último método es la distancia de Chebyshev, esta es calculada de la siguiente manera 
\begin{align*} 
    d_{agent} = \max(|x_{agent} - x_{goal}|, |y_{agent} - y_{goal}|)
\end{align*}



Aquí, $(x_{\text{agent}}, y_{\text{agent}})$ son las coordenadas actuales del agente, y $(x_{\text{goal}}, y_{\text{goal}})$ son las coordenadas de cada punto objetivo en la lista de objetivos. Esta recompensa incentiva al agente a acercarse lo más posible al objetivo.\\
\end{itemize}

Combinando los términos anteriores, la función de aptitud se expresa como:
\begin{align*} 
Aptitud = 10000 - d_{agent} 
\end{align*}
Además, para asegurar que el valor de aptitud no sea negativo, se aplica:
\begin{align*} 
Aptitud = \max(0, Aptitud) 
\end{align*}

Entre mayor sea la aptitud, los genomas con sus características aumentan las probabilidades de ser seleccionados para la próxima generación.
%%%
\begin{figure}
    \centering
    \includegraphics[scale=0.14]{images/gta2.png}
    \caption{Mapa del entorno donde el agente autónomo realizará su aprendizaje}
    \label{fig:Mapa}
\end{figure} 

\subsection{Implementación de asignación entrada  salida}
Dadas las entradas de los sensores, se le asigna un angulo de rotación y una velocidad al agente autónomo, es un esquema básico, pero funcional. %ver Figura \ref{fig:Bucle_basico}.
Sin embargo, que pasa si las variaciones de los ángulos son muy altas, o tal vez muy bajas, o quizá la velocidad es muy lenta.
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{images/bucle_inicial.png}
%     \caption{Bucle básico de como el agente autónomo es retroalimentado con las entradas}
%     \label{fig:Bucle_basico}
% \end{figure} 
 Si el agente recibe una entrada constante de ángulos en una misma dirección, ocasiona que la suma constante de esos ángulos lo haga girar, más si dichas entradas se presentan en las primeras generaciones. Por contraparte, su desplazamiento se ve disminuido, lo que implica un estancamiento del agente. En el caso de que la entrada sea demasiada alterada pero iterando la dirección de los ángulos, su velocidad también es reducida, ya que si no lo hace, el carro será eliminado debido al inminente hecho de chocar con los bordes fuera de la zona establecida como carretera.

Para evitar alguna de las singularidades (asúmase singularidad como estancamiento, rotación sin parar) se asigna un \textbf{Set\_Valor} a la velocidad si la velocidad previa del agente es menor a 0.1, esto evita que el agente avance demasiado lento, optimizando el tiempo de cada simulación, en este articulo, \textbf{Set\_Valor} es igual a 5. A esta asignación la denominamos \textit{Refuerzo forzado} o \textit{Método de aceleración}, y su diagrama es presentado en la Figura \ref{fig:reforce}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/reforce.png}
    \caption{Diagrama de flujo de la función de aceleración o refuerzo forzado, diseñada para optimizar y reducir el estancamiento de algunos agentes en primeras generaciones.}
    \label{fig:reforce}
\end{figure}

Una vez establecidos los parámetros iniciales, se realizan distintas simulaciones con el fin de recopilar datos para analizar el comportamiento de los agentes. Se realizaron un total de 63 simulaciones para el primer mapa (Figura \ref{fig:Mapa}), distribuidas en grupos de tres, simulaciones para la distancia Manhattan, Euclidiana y Chebyshev. A cada número de generación le corresponden 5 simulaciones, es decir que en la distancia Euclidiada fue un total de $3*5 = 15$ simulaciones realizadas, donde 5 fueron las simulaciones realizadas con una generación de 20, otras 5 para la generación de 30 y por último otras 5 para la generación de 50; así para la distancia Manhattan y Chebyshev. Estas fueron realizadas con la aplicación del método \textbf{Refuerzo forzado} (ver Figura \ref{fig:reforce}), además se realizaron tres simulaciones adicionales sin este método. Corresponden a una simulación de 50 generaciones para las tres distancias aplicadas, por ende el total de simulaciones son las 45 contadas anteriormente, más 3 simulaciones sin la aplicación de este método, como se observa en la Tabla \ref{tab:fds_1}.


\begin{table}
\centering
\caption{}
\label{tab:fds_1}
\begin{tabular}{lll}
\multicolumn{3}{c}{\texttt{DISTRUBUCIÓN DE LOS ENTRENAMIENTOS}}
\\
\toprule
\textbf{Simulaciones} & 
\textbf{Generaciones}  &
\textbf{Refuerzo forzado}\\
\midrule
Euclidiana (5)   &  20, 30, 50 & SI\\
Manhattan (5)     &  20, 30, 50 & SI\\
Chebyshev (5)     &  20, 30, 50 & SI\\
\bottomrule
Euclidiana (1)    & 50 & NO\\
Manhattan (1)     & 50 & NO\\
Chebyshev  (1)    & 50 & NO\\
\bottomrule
\end{tabular}
\end{table}

También se realizaron 15 simulaciones adicionales para un segundo mapa ver (Fig. \ref{fig:mapa2}), en estas simulaciones solo se realizaron para una generación de 50, como se indica en la Tabla \ref{tab:fds_2}, donde (5) representan las simulaciones realizadas, siguiendo el orden planteado anteriormente, 5 simulaciones realizadas para la distancia Euclidiana con una generación de 50, y así para la distancia Manhattan y Chebyshev, para un total de $5*3=15$ simulaciones.

\begin{table}
\centering
\caption{}
\label{tab:fds_2}
\begin{tabular}{lll}
\multicolumn{3}{c}{\texttt{DISTRUBUCIÓN DE LOS ENTRENAMIENTOS MAPA 2}}
\\
\toprule
\textbf{Simulaciones} & 
\textbf{Generaciones}  &
\textbf{Refuerzo forzado}\\
\midrule
Euclidiana (5)   &50 & SI\\
Manhattan (5)     &50 & SI\\
Chebyshev (5)     &50 & SI\\
\bottomrule
\end{tabular}
\end{table}

% \begin{figure}
%     \centering    \includegraphics[width=0.8\linewidth]{Simulacion/gta.png}
%     \caption{Mapa con bloqueo de vía inferior}
%     \label{fig:mapa2}
% \end{figure}

\section{Resultados}
Los registros obtenidos de las simulaciones contienen el fitness promedio de los genomas, el mejor fitness de la simulación, por últimos las máximas y mínimas recompensas. Con estos datos, se obtiene la gráfica el fitness promedio con las desviaciones presentadas.

% \subsection{Configuración del mapa implementado en la simulación}
% Durante la preparación del mapa para la simulación, a los bordes principales de la carretera se les asigno un color RGB(20, 23.5, 21.6). Sin embargo, en la configuración para determinar los limites de la vía se opta por establecer que los limites son todos los colores distintos del negro.

\subsection{Configuraciones principales del algoritmo NEAT}
El archivo de configuración para NEAT consta de distintas secciones, cada una de ellas orientada a como será la creación de generaciones en cada simulación. En la sección [NEAT] \ref{tab:NEAT_1}, se pueden encontrar los parámetros fitness y el tamaño de la población en cada generación; en la sección [DefaultReproduction] \ref{tab:DefaultReproduction}, podemos encontrar la configuración de elitism y survival threshold, que permiten determinar el numero de especies mas aptas de cada generación que se desea conservar, y la proporción de cada especie permitida para reproducirse en cada generación; en la sección [DefaultGenome] \ref{tab:DefaultGenome_1}, se define los parámetros específicos para la estructura de los genemoas (redes neuronales) que Neat evolucionará, como el modo de activación o activation defauld y activation mutate rate vea la tabla completa en Tabla \ref{tab:DefaultGenome}.
\begin{table}[H]
    \centering
    % Primera tabla
    \captionof{table}{\texttt{[NEAT]}}
    \label{tab:NEAT_1}
    \begin{tabular}{ll}
    \toprule
    \textbf{Variable} & \textbf{Valor} \\
    \midrule
    fitness\_criterion     & max \\
    fitness\_threshold     & 10000 \\
    pop\_size              & 50 \\
    reset\_on\_extinction  & True  \\
    \bottomrule
    \end{tabular}
    \vspace{0.5cm}
    
    % % Segunda tabla
    % \captionof{table}{\texttt{[DefaultReproduction]}}
    % \label{tab:DefaultReproduction}
    % \begin{tabular}{ll}
    % \toprule
    % \textbf{Variable} & \textbf{Valor} \\
    % \midrule
    % excess\_coeff            & 1.0 \\
    % disjoint\_coeff          & 1.0 \\
    % weight\_diff\_coeff      & 0.5 \\
    % compatibility\_threshold & 3.0 \\
    % elitism                  & 5 \\
    % survival\_threshold      & 0.2 \\
    % \bottomrule
    % \end{tabular}
    % \vspace{0.5cm}

    \captionof{table}{\texttt{[DefaultGenome]}}
    \label{tab:DefaultGenome_1}
    \begin{tabular}{ll}
    \toprule
    \textbf{Opciones de Activación de Nodos} & \textbf{Valor} \\
    \midrule
    activation\_default       & tanh \\
    activation\_mutate\_rate  & 0.01 \\
    activation\_options       & tanh, relu, sigmoid \\
    \bottomrule
    \end{tabular}

\end{table}

% \begin{table}[ht]
%     % Tercera tabla
%     \centering
%     \captionof{table}{\texttt{[DefaultGenome]}}
%     \label{tab:DefaultGenome}
%     \begin{tabular}{ll}
%     \toprule
%     \textbf{Variable} & \textbf{Valor} \\
%     \midrule
%     \multicolumn{2}{l}{\textbf{Opciones de Activación de Nodos}} \\
%     activation\_default       & tanh \\
%     activation\_mutate\_rate  & 0.01 \\
%     activation\_options       & tanh, relu, sigmoid \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Opciones de Agregación de Nodos}} \\
%     aggregation\_default      & sum \\
%     aggregation\_mutate\_rate & 0.01 \\
%     aggregation\_options      & sum \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Opciones de Sesgo de Nodos}} \\
%     bias\_init\_mean         & 0.0 \\
%     bias\_init\_stdev        & 1.0 \\
%     bias\_max\_value         & 30.0 \\
%     bias\_min\_value         & -30.0 \\
%     bias\_mutate\_power      & 0.5 \\
%     bias\_mutate\_rate       & 0.7 \\
%     bias\_replace\_rate      & 0.1 \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Opciones de Compatibilidad del Genoma}} \\
%     compatibility\_disjoint\_coefficient & 1.0 \\
%     compatibility\_weight\_coefficient   & 0.5 \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Tasas de Adición/Eliminación de Conexiones}} \\
%     conn\_add\_prob        & 0.5 \\
%     conn\_delete\_prob     & 0.5 \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Opciones de Habilitación de Conexiones}} \\
%     enabled\_default       & True \\
%     enabled\_mutate\_rate  & 0.1 \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Configuraciones de Topología}} \\
%     feed\_forward          & False \\
%     initial\_connection    & full \\
%     \addlinespace
    
%     \multicolumn{2}{l}{\textbf{Tasas de Adición/Eliminación de Nodos}} \\
%     node\_add\_prob        & 0.2 \\
%     node\_delete\_prob     & 0.2 \\
%     \bottomrule
%     \end{tabular}
% \end{table}

Durante las simulaciones realizadas se almacenaron los datos del fitness correspondiente, las primeras 5 simulaciones realizadas correspondieron a un número de 50 generaciones, luego 5 para 30 generaciones y por ultimo 5 para 20 generaciones, un total de 15 simulaciones realizadas para cada distancia.
Como resultados se obtienen las graficas presentadas en las Figuras \ref{fig:graficas_generales_euclidiana}, \ref{fig:graficas_generales_manhattan} y \ref{fig:graficas_generales_chebyshev}. La zona sombreada corresponde a las desviaciones de cada simulación, estas desviaciones indican agentes alejados del promedio. Basándose en ello se puede observar que simulaciones obtuvieron la mayor cantidad de puntos.

\subsection{Resultados de las simulaciones usando la distancia Euclidiana}
Con un fitness maximo total obtenido para la distancia Euclidiana de 9990, se obtuvo en el entrenamiento 4 con 50 generaciones (Fig. \ref{fig:Fitnes_ecu_4_50_inv} y \ref{fig:Fitnes_ecu_4_50_inv_sombra}), seguido de este entrenamiento, el entrenamiento 3 obtuvo el segundo fitness mas alto con 9986 para 30 generaciones (Fig. \ref{fig:Fitnes_ecu_3_30_inv} y \ref{fig:Fitnes_ecu_3_30_inv_sombra})
, y por último el entrenamiento 2 obtuvo el tercer fitness mas alto con 9881, con 20 generaciones (Fig. \ref{fig:eucli_2_20} y \ref{fig:eucli_2_20_sombra}). La diferencia en 50 genraciones y 30 generaciones es de 4 puntos, mientras que la diferencia del fitness maximo para las 50 generaciones y 20 generaciones es de 109 puntos

% Para los entrenamientos realizados con 50 generaciones, el entrenamiento 4 obtuvo el fitness máximo, con un valor exacto de 9990, como se aprecia en la Figura \ref{fig:Fitnes_ecu_4_50_inv}. Ademas, este entrenamiento presentó la mayor desviación, como se observa en la Figura \ref{fig:Fitnes_ecu_4_50_inv_sombra}.

%El segundo fitness más alto le corresponde al entrenamiento 1, con un valor de 9988, además otra característica para el primer entrenamiento corresponde a la desviación, esta sobrepasa los 9400, sin embargo respecto al cuarto entrenamiento su desviación son 200 puntos por debajo (Fig. \ref{fig:eucli_1_50} y \ref{fig:eucli_1_50_sombra}). Seguido de este entrenamiento, el tercer entrenamiento obtuvo un fitness de 9985, con una desviación de 9600, misma desviación que el cuarto entrenamiento (Fig. \ref{fig:eucli_3_50} y \ref{fig:eucli_3_50_sombra}). El quinto entrenamiento obtuvo un fitness de 9982, con una desviación de 9400, en el mismo rango de desviación que el primer entrenamiento (Fig. \ref{fig:eucli_5_50} y \ref{fig:eucli_5_50_sombra}). Para finalizar, el entrenamiento 2 obtuvo el fitnes más bajo con un puntaje de 9626, con la menor desviación de las 5 simulaciones (Fig. \ref{fig:eucli_2_50} y \ref{fig:eucli_2_50_sombra}).

%%%%%%%%%%%%%%%%

% (Fig. \ref{fig:Fitnes_ecu_3_30_inv} y Fig. \ref{fig:Fitnes_ecu_3_30_inv_sombra})

% Para los datos recopilados con un número de generaciones igual a 30, la gráfica obtenida se observa en la Figura \ref{fig:graficas_generales_euclidiana} sección \textit{(B) 30 Genraciones}. Los entrenamientos que presentan las mayores desviaciones fueron el primer, tercer y quinto entrenamiento. El fitness máximo corresponde al tercer entrenamiento con 9986 como se aprecia en la Figura \ref{fig:Fitnes_ecu_3_30_inv}, ademas presentó la mayor desviación de todas para esta sección, con un valor de 9400 (ver Figura \ref{fig:Fitnes_ecu_3_30_inv_sombra}).

% El segundo fitness mas alto le corresponde al entrenamiento 5, con un valor de 9984, seguido de este entrenamiento, el primer entrenamiento obtuvo un fitness de 9981, con una desviación que se aproxima a los 9100, 100 puntos menos que la desviación correspondiente al quinto entrenamiento (Fig. \ref{fig:eucli_5_30}, \ref{fig:eucli_5_30_sombra}, \ref{fig:eucli_1_30} y  \ref{fig:eucli_1_30_sombra}).

% La gráfica general de las desviaciones y promedios obtenidos para 20 genraciónes establecidas puede observarla en la sección \textit{(C) 20 Genraciones} en la Figura \ref{fig:graficas_generales_euclidiana}.
% Como se puede observar, la desviación con mayor valor se obtuvo en el segundo entrenamiento, además su puntaje fitness fue el mas alto de todos los demás con un valor de 9881, con una desviación que no sobrepasa los 9100 puntos (Fig. \ref{fig:eucli_2_20} y \ref{fig:eucli_2_20_sombra}).

% Con un fitness maximo total obtenido para la distancia Euclidiana de 9990, se obtuvo en el entrenamiento 4 con 50 generaciones, seguido de este entrenamiento, el entrenamiento 3 obtuvo el segundo fitness mas alto con 9986 para 30 generaciones, y por último el entrenamiento 2 obtuvo el tercer fitness mas alto con 9881, con 20 generaciones. La diferencia en 50 genraciones y 30 generaciones es de 4 puntos, mientras que la diferencia del fitness maximo para las 50 generaciones y 20 generaciones es de 109 puntos. 

La Tabla \ref{tab:fitness_generaciones} contine los resultados obtenidos para las simulaciones realizadas con la distancia Euclidiana, donde se presentan los fitness maximos obtenidos para cada entrenamiento.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=2.74cm]{Euclidiana/Fitness_Acumulado_Eucli_50Gen.png}
        \caption{50 Generaciones}
        \label{fig:eucli_50gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=2.74cm]{Euclidiana/Fitness_Acumulado_Eucli_30Gen.png}
        \caption{30 Generaciones}
        \label{fig:eucli_30gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=2.74cm]{Euclidiana/Fitness_Acumulado_Eucli_20Gen.png}
        \caption{20 Generaciones}
        \label{fig:eucli_20gen}
    \end{subfigure}
	\caption{Gráficas por generaciones para la distancia Euclidiana}
    \label{fig:graficas_generales_euclidiana}
\end{figure*}

\subsection{Resultados de las simulaciones aplicando la distancia Manhattan}
El valor de fitness maximo obetnido en la distancia Manhattan corresponde al entrenamiento numero 1 con 9990 para 30 generaciones  (Fig. \ref{fig:manhattan_1_30} y \ref{fig:manhattan_1_30_sombra}), seguido de este entrenamiento, el entrenamiento 2 obtuvo el segundo fitness mas alto con 9980 para 20 generaciones (Fig. \ref{fig:manhattan_2_20} y \ref{fig:manhattan_2_20_sombra}), y por último el entrenamiento 4 obtuvo el tercer fitness mas alto con 9978, con 50 generaciones  (Fig \ref{fig:manhattan_4_50} y \ref{fig:manhattan_4_50_sombra}). 
La diferencia entre el fitness maximo obtenido para 30 generaciones y 20 generaciones es de 10 puntos, mientras que la diferencia del fitness maximo para las 30 generaciones y 50 generaciones es de 12 puntos.


% Principalmente se iniciaron las simulaciones para las 50 generaciones, de estas simulaciones se graficaron los mejores promedios y las desviaciones de cada entrenamiento, de esta manera la grafica que reúne cada uno de los entrenamientos es la Figura \ref{fig:graficas_generales_manhattan} \textit{(A) 50 Generaciones}.
% En el entrenamiento número 4 se presentó el fitness mas alto con 9978 junto con las desviaciones mas altas de todos los entrenamientos, de aproximadamente 9100 puntos, con una desviación cercana a los 9200 (Fig \ref{fig:manhattan_4_50} y \ref{fig:manhattan_4_50_sombra}).

% En la gráfica general para las simulaciones con 30 generaciones se observa los fitness promedio de cada entrenamiento y las desviaciones ver la Figura \ref{fig:graficas_generales_manhattan} \textit{(B) 30 Generaciones}.
% El primer entrenamiento presento el fitness mas alto de todos los entrenamientos, con un puntaje de 9990, con una desviación que supera los 9200 puntos (Fig. \ref{fig:manhattan_1_30} y \ref{fig:manhattan_1_30_sombra}).


% En la gráfica general para las simulaciones con 20 generaciones se observan los fitness promedio de cada entrenamiento y las desviaciones Figura \ref{fig:graficas_generales_manhattan} \textit{(C) 20 Generaciones}.
% El entrenamiento número dos obtuvo el mejor fitness, con un puntaje de 9980, este entrenamiento también posee la mayor desviación de todas, sin embargo esta desviación no suoera los 8900 puntos (Fig. \ref{fig:manhattan_2_20} y \ref{fig:manhattan_2_20_sombra}). El entrenamiento número 4 obtuvo un fitness maximo de 8879, sin embargo su fitness promedio y desviación empezó a disminuir a partir de la genreación numero 15 ver (Fig. \ref{fig:manhattan_4_20_sombra})

% El valor de fitness maximo obetnido en la distancia Manhattan corresponde al entrenamiento numero 1 con 9990 para 30 generaciones, seguido de este entrenamiento, el entrenamiento 2 obtuvo el segundo fitness mas alto con 9980 para 20 generaciones, y por último el entrenamiento 4 obtuvo el tercer fitness mas alto con 9978, con 20 generaciones. La diferencia entre el fitness maximo obtenido para 30 generaciones y 20 generaciones es de 10 puntos, mientras que la diferencia del fitness maximo para las 30 generaciones y 50 generaciones es de 12 puntos.


La Tabla \ref{tab:fitness_generaciones_manhattan} contine los resultados obtenidos para las simulaciones realizadas con la distancia Manhattan, donde se presentan los fitness maximos obtenidos para cada entrenamiento.
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Manhattan/Fitness_Acumulado_Manh_50Gen.png}
        \caption{50 Generaciones}
        \label{fig:manh_50gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Manhattan/Fitness_Acumulado_Manh30Gen.png}
        \caption{30 Generaciones}
        \label{fig:manh_30gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=2.74cm]{Manhattan/Fitness_Acumulado_Manh_20Gen.png}
        \caption{20 Generaciones}
        \label{fig:manh_20gen}
    \end{subfigure}
    \caption{Gráficas por generaciones para la distancia Manhattan}
    \label{fig:graficas_generales_manhattan}
\end{figure*}



\subsection{Resultados de las simulaciones aplicando la distancia de Chebyshev}
El valor de fitness maximo obtenido en la distancia Chebyshev corresponde al entrenamiento numero 2 con 9994 para 30 generaciones (Fig. \ref{fig:cheb_2_30} y \ref{fig:cheb_2_30_sombra}), seguido de este entrenamiento, el entrenamiento 2 obtuvo el segundo fitness mas alto con 9990 para 20 generaciones (Fig. \ref{fig:cheb_2_20} y \ref{fig:cheb_2_20_sombra}), y por último el entrenamiento 1 obtuvo el tercer fitness mas alto con 9988, con 50 generaciones (Fig \ref{fig:cheb_1_50} y \ref{fig:cheb_1_50_sombra}).
La diferencia entre el fitness maximo obtenido para 30 generaciones y 20 generaciones es de 4 puntos, mientras que la diferencia del fitness maximo para las 30 generaciones y 50 generaciones es de 6 puntos.

% Para las simulaciones realizadas con la distancia de Chebyshev, se obtuvieron los resultados de las gráficas generales para las 50 generaciones, 30 generaciones y 20 generaciones, como se observa en la Figura \ref{fig:graficas_generales_chebyshev}.
% La grafica que reúne cada uno de los entrenamientos realizados para las 50 Generaciones es la Figura \ref{fig:graficas_generales_chebyshev} \textit{(A) 50 Generaciones}.
% En el entrenamiento número 1 se presentó el fitness mas alto con 9988 junto con las desviaciones mas altas de todos los entrenamientos, de aproximadamente 9400 puntos (Fig \ref{fig:cheb_1_50} y \ref{fig:cheb_1_50_sombra}).

% En la gráfica general para las simulaciones con 30 generaciones se observa los fitness promedio de cada entrenamiento y las desviaciones ver la Figura \ref{fig:graficas_generales_chebyshev} \textit{(B) 30 Generaciones}.
% El segundo entrenamiento presento el fitness mas alto de todos los entrenamientos, con un puntaje de 9994, con una desviación que supera los 9400 puntos (Fig. \ref{fig:cheb_2_30} y \ref{fig:cheb_2_30_sombra}).


% En la gráfica general para las simulaciones con 20 generaciones se observan los fitness promedio de cada entrenamiento y las desviaciones Figura \ref{fig:graficas_generales_chebyshev} \textit{(C) 20 Generaciones}.
% El entrenamiento número dos obtuvo el mejor fitness, con un puntaje de 9990, este entrenamiento también posee la mayor desviación de todas, sin embargo esta desviación no supera los 9100 puntos (Fig. \ref{fig:cheb_2_20} y \ref{fig:cheb_2_20_sombra}).

% El valor de fitness maximo obetnido en la distancia Chebyshev corresponde al entrenamiento numero 2 con 9994 para 30 generaciones, seguido de este entrenamiento, el entrenamiento 2 obtuvo el segundo fitness mas alto con 9990 para 20 generaciones, y por último el entrenamiento 1 obtuvo el tercer fitness mas alto con 9988, con 50 generaciones.
% La diferencia entre el fitness maximo obtenido para 30 generaciones y 20 generaciones es de 4 puntos, mientras que la diferencia del fitness maximo para las 30 generaciones y 50 generaciones es de 6 puntos.

La Tabla \ref{tab:fitness_generaciones_chebyshev} contine los resultados obtenidos para las simulaciones realizadas con la distancia Manhattan, donde se presentan los fitness maximos obtenidos para cada entrenamiento.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Chebyshev/Fitness_Acumulado_Cheby_50Gen.png}
        \caption{50 Generaciones}
        \label{fig:cheb_50gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Chebyshev/Fitness_Acumulado_Cheby30Gen.png}
        \caption{30 Generaciones}
        \label{fig:cheb_30gen}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Chebyshev/Fitness_Acumulado_Cheby_20Gen.png}
        \caption{20 Generaciones}
        \label{fig:cheb_20gen}
    \end{subfigure}
    \caption{Gráficas por generaciones para la distancia Chebyshev}
    \label{fig:graficas_generales_chebyshev}
\end{figure*}

En la Tabla \ref{tab:fitness_generaciones_maximos_minimos} se presentan los fitness maximos y minimos obtenidos para cada distancia, en cada generación, en la columna \textbf{Mejor Fitness}, se toma al agente con el mayor figness obtenido en la ultima generación, mientras que en \textbf{Mejor Fitness}, el menor. Durante un entrenamiento de 50 generaciones, en cada genración, sea la pirmera, luego la segunda y así sucesivamente, se obtiene un promedio fitness de todos los agentes generados, ese promedio general se presenta en la columna \textbf{Promedio}.
\begin{table}[ht]
\centering
\caption{}
\label{tab:fitness_generaciones_maximos_minimos}
\begin{tabular}{c c c c c}
%\hline
\multicolumn{5}{c}{\texttt{FITNESS MAXIMOS, PROMEDIOS Y MINIMOS}}\\\toprule
 & \multicolumn{1}{c}{Gen.} &\multicolumn{1}{c}{Mejor Fitness} & \multicolumn{1}{c}{Promedio} & \multicolumn{1}{c}{Menor Fitness} \\ \toprule

\multirow{2}{*}{Euclidiada} & 50 & 9990 & 8898 & 9626\\ 
                            & 30 & 9986 & 8718 & 8544\\
                            & 20 & 9981 & 8631 & 8773\\ \toprule

\multirow{3}{*}{Manhattan}  & 50 & 9978 & 8544 & 9621\\
                            & 30 & 9990 & 8558 & 9618\\ 
                            & 20 & 9980 & 8489 & 8766\\ \toprule

\multirow{3}{*}{Chebyshev}  & 50 & 9988 & 8803 & 8777\\ 
                            & 30 & 9994 & 8708 & 8787\\ 
                            & 20 & 9990 & 8655 & 8543\\ \toprule

\end{tabular}
\end{table}

\subsection{Entrenamiento sin el Refuerzo forzado}
Para esta sección solo se realizó un entrenamiento de 50 generaciones para cada metrica de distancia con el fin de analizar que tan optimo era realizar las simulaciones sin la aplicación del método descrito en la Figura \ref{fig:reforce}.
% Ch 9885
% mah 8535
En la aplicación de la distancia Euclidiana el mayor fitness obtenido es de 8536 (Fig. \ref{fig:eucli_no_ref_50}), por otra parte la desviación obtenida describe el comportamiento de los agentes en cada generación, cómo podrá observarse en la Figura \ref{fig:no_refoice_eucli_desv}, el mayor incremento de fitness solo se obtuvo en la genracion 10, luego de la generacion 12 no incrementaba el aprendizaje en los agentes.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Sin_Refuerzo_50Gen/Eucli_NoRef_50Gen_Sombra.png}
    \caption{Desviación y promedio fitness para la distancia Euclidiana sin la aplicación del Refuerzo forzado}
    \label{fig:no_refoice_eucli_desv}
\end{figure}

Implementando la distancia Manhattan el puntaje solo se diferencia en 1 de la distancia Euclidiana, con un puntaje de 8535, la grafica de desviación mantuvo el mismo comportamiento que la distancia Euclidiana, sin embargo la desviación en la genración 10 superó los 8625 puntos (Fig. \ref{fig:manh_no_ref_50} y \ref{fig:manh_no_ref_50_sombra}).

Por último, en esta sección la distancia de Chebyshev presentó los mejores resultados, con un puntaje fitness máximo obtenido de 9885 (Fig. \ref{fig:cheby_no_ref_50}), y una desviación que ronda los 8800 puntos. La diferencia principal con respecto a las otras metricas de distancias radica en el aprendizaje constante de los agentes con cada genración, como se observa en la Figura \ref{fig:no_refoice_chy_desv}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Sin_Refuerzo_50Gen/Cheby_NoRef_50Gen_Sombra.png}
    \caption{Desviación y promedio fitness para la distancia Chebyshev sin la aplicación del Refuerzo forzado}
    \label{fig:no_refoice_chy_desv}
\end{figure}

\subsection{Simulaciones de 50 generaciones realizadas en el segundo mapa con el Refuerzo Forzado}
Se realizaron 5 simulaciones de 50 genraciónes para cada sistema de recompensas en el segundo mapa ver figura \ref{fig:mapa2}, Euclidiana, Manhattan y Chebyshev. Con los datos recopilados de estas simulaciones, se analizan las gráficas generales, y se presentan los resultados obtenidos en la Tabla \ref{tab:fitness_generaciones_maximos_minimos_mapa2}. La gráfica general obtenida después de cada entrenamiento para las distintas distancias la puede observar en Fig. \ref{fig:graficas_generales_mapa2}.

\begin{table}[ht]
    \centering
    \caption{}
    \label{tab:fitness_generaciones_maximos_minimos_mapa2}
    \begin{tabular}{c c c c c}
    %\hline
    \multicolumn{5}{c}{\texttt{FITNESS MAXIMOS, PROMEDIOS Y MINIMOS SEGUNDO MAPA}}\\\toprule
     & \multicolumn{1}{c}{Gen.} &\multicolumn{1}{c }{Mejor Fitness} & \multicolumn{1}{c}{Promedio} & \multicolumn{1}{c}{Menor Fitness} \\ \toprule
    
    \multirow{1}{*}{Euclidiada} & 50 & 9684 & 8686 & 8933\\\toprule    
    \multirow{1}{*}{Manhattan}  & 50 & 9890 & 8534 & 8532\\ \toprule
    \multirow{1}{*}{Chebyshev}  & 50 & 9683 & 8694 & 8985\\ \toprule
    
    \end{tabular}
\end{table}
    


\subsubsection{Fitness para la distancia Euclidiana, Manhattan y Chebyshev}
El mejor fitness obtenido es de 9684, por el tercer y quinto entrenamiento. El menor fitness fue obtenido por el cuarto entrenamiento con 8933 puntos (Fig. \ref{fig:eucli_3_50_m2}, \ref{fig:eucli_3_50_sombra_m2}, \ref{fig:eucli_4_50_m2} y \ref{fig:eucli_4_50_sombra_m2}).

Para la distancia Manhattan el fitness más alto obtenido en estas simulaciones realizadas para es para el entrenamiento numero 2, con un puntaje máximo de 9890 y una desviación que supera los 8900 puntos.  El primer entrenamiento obtuvo el puntaje mas bajo con 8532. Las desviaciones del primer entrenamiento no pasan de los 8550 puntos (Fig. \ref{fig:manh_2_50_m2}, \ref{fig:manh_2_50_sombra_m2}, \ref{fig:manh_1_50_m2} y \ref{fig:manh_1_50_sombra_m2}).

El entrenamiento que mayor fitness obtuvo en la distancia Chebyshev corresponde al cuarto entrenamiento con un puntaje de 9683, con una desviación que se acerca a los 9100 puttos. El fitness mas bajo obtenido le corresponde el entrenamietno numero 5 con 8985 puntos y un decrecisimiento en la desviación a partir de la genración 30 (Fig. \ref{fig:cheb_4_50_m2}, \ref{fig:cheb_4_50_sombra_m2}, \ref{fig:cheb_5_50_m2} y \ref{fig:cheb_5_50_sombra_m2}).

% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=0.9\linewidth]{Euclidiana/Mapa2/Fitness_Prrm_Map2_Eucli_50Gen.png}
%         \caption{Distancia Euclidiana}
%         \label{fig:ecuclidiana_mapa2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=0.9\linewidth]{Manhattan/Mapa2/Fitness_Prom_Map2_Manh_50Gen.png}
%         \caption{Distancia Manhattan}
%         \label{fig:manhattan_mapa2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.3\textwidth}
%         \centering
%         \includegraphics[width=0.9\linewidth]{Chebyshev/Mapa2/Fitness_Prom_Map2_Cheby_50Gen.png}
%         \caption{Distancia Chebyshev}
%         \label{fig:chebyshev_mapa2}
%     \end{subfigure}
%     \caption{Gráficas generales de promedios fitness y desviaciones para el segundo mapa}
%     \label{fig:graficas_generales_mapa2}
% \end{figure*}


\section{Conclusiones}
La implementación de la librería NEAT para un vehículo autónomo permitió analizar distintos aspectos del aprendizaje y desempeño del agente en diversos entornos. En este caso se realizaron dos pruebas, el entorno libre de obstáculos, mientras que en la otra se añadió un bloqueo de ruta. Se observó que el agente mostró un mejor rendimiento en el entorno libre de obstáculos, en comparación del entorno con la ruta bloqueada. Al agente le tomaba más generaciones aprender en comparación cuándo alguna vías era obstruida. En cuanto a las métricas de distancia, la distancia Chebyshev demostró ser más óptima que la Manhattan y Euclidiana en entornos libres de obstáculos. Sin embargo, en escenarios con obstáculos, la distancia Euclidiana ofreció un mejor desempeño obteniendo un alto valor fitness. La distancia Euclidiana aumentaba al igual que la de Chebyshev con el pasar de las generaciones, sin embargo la distancia Manhattan obtuvo su mayor fitness con una generación de 30. Con los resultados obtenidos se puede concluir que para la distancia Manhattan y Chebyshev con 30 generaciones ya se obtiene un buen fitness.

Por otro lado, la implementación del refuerzo forzado generó mejoras importantes, como disminuir el tiempo requerido para que el agente alcanzara un fitness significativo. Sin la aplicación del refuerzo forzado, el agente no lograba obtener fitness altos, lo que se traduce en un aprendizaje más lento y menos eficiente. Sin embargo la distancia de Chebyshev demostró ser las más eficiente sin la aplicación del refuerzo forzado, obteniendo un fitness de 9885, superando a la Euclidiana y Manhattan. La metrica con los mejores resultados generales fue la distancia Chebyshev, demostrando el incremeto del fitness en cada generación, con la aplicación o no del Refuerzo Forzado.
\printbibliography
\vspace{-2cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/JPH}}]{Juan~P.~Hoyos}
Received the Engineer, Magister in Electronics and Telecommunications and Doctor in Electronic Sciences degrees from Universidad del Cauca, Popayan, Colombia, in 2010, 2016, and 2018 respectively. From 2019 to 2021, he was a Postdoctoral fellow at the GNTT of the of the Universidad del Cauca, and from 2020 to 2022 he was a Postdoctoral Researcher at the Center for Mathematical Modeling (CMM) at Universidad de Chile. He is currently an assistant professor at the Universidad Nacional de Colombia, sede De La Paz. His research interests are in signal processing, artificial intelligence, and convex optimization.
\end{IEEEbiography}
\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/JPH}}]{Juan~P.~Hoyos}
Received the Engineer, Magister in Electronics and Telecommunications and Doctor in Electronic Sciences degrees from Universidad del Cauca, Popayan, Colombia, in 2010, 2016, and 2018 respectively. From 2019 to 2021, he was a Postdoctoral fellow at the GNTT of the of the Universidad del Cauca, and from 2020 to 2022 he was a Postdoctoral Researcher at the Center for Mathematical Modeling (CMM) at Universidad de Chile. He is currently an assistant professor at the Universidad Nacional de Colombia, sede De La Paz. His research interests are in signal processing, artificial intelligence, and convex optimization.
\end{IEEEbiography}
\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/JPH}}]{Juan~P.~Hoyos}
Received the Engineer, Magister in Electronics and Telecommunications and Doctor in Electronic Sciences degrees from Universidad del Cauca, Popayan, Colombia, in 2010, 2016, and 2018 respectively. From 2019 to 2021, he was a Postdoctoral fellow at the GNTT of the of the Universidad del Cauca, and from 2020 to 2022 he was a Postdoctoral Researcher at the Center for Mathematical Modeling (CMM) at Universidad de Chile. He is currently an assistant professor at the Universidad Nacional de Colombia, sede De La Paz. His research interests are in signal processing, artificial intelligence, and convex optimization.
\end{IEEEbiography}
\vspace{-2.5cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/JPH}}]{Juan~P.~Hoyos}
Received the Engineer, Magister in Electronics and Telecommunications and Doctor in Electronic Sciences degrees from Universidad del Cauca, Popayan, Colombia, in 2010, 2016, and 2018 respectively. From 2019 to 2021, he was a Postdoctoral fellow at the GNTT of the of the Universidad del Cauca, and from 2020 to 2022 he was a Postdoctoral Researcher at the Center for Mathematical Modeling (CMM) at Universidad de Chile. He is currently an assistant professor at the Universidad Nacional de Colombia, sede De La Paz. His research interests are in signal processing, artificial intelligence, and convex optimization.
\end{IEEEbiography}


\end{document}